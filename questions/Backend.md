# Server Questions

## 🤷🏻‍♂️ APM(Application Performance Monitoring)을 활용하여 배포 후 성능 저하가 발생한 경우, 이를 어떻게 식별하고 해결하셨는지 구체적인 사례를 설명해주세요.

### 🙆🏻‍♂️ 답변
> 신규 기능 배포 후 APM을 통해 특정 API의 응답 시간이 급증한 것을 발견했습니다. APM에서 SQL 쿼리의 실행 시간이 길어지는 것을 포착했고, 인덱스 최적화와 캐싱을 적용하여 문제를 해결했습니다. 이후 응답 시간이 정상화되었고, 배포 전 테스트 케이스를 추가하여 유사한 문제가 재발하지 않도록 조치했습니다.

### 🔑 키워드
> 장애 대응, 모니터링, 로깅, APM, 성능 저하, SQL 최적화, 캐싱, 모니터링

<hr>

## 🤷🏻‍♂️ 최근 발생했던 시스템 장애 중 가장 어려웠던 케이스를 설명하고, 이를 해결하기 위해 사용한 모니터링 도구와 그 도구를 어떻게 활용했는지 설명해 주세요.

### 🙆🏻‍♂️ 답변
> 한 번은 서버 응답 시간이 급증했는데, 원인을 파악하기 어려웠습니다. Grafana와 Prometheus를 활용하여 애플리케이션의 메트릭을 수집하고 시각화했으며, 이를 통해 특정 API 호출이 비정상적으로 증가한 것을 발견했습니다. <br> 집중적으로 해당 API의 로그를 분석하여 문제를 해결할 수 있었습니다. 사전 설정한 알람 기능 덕분에 빠르게 대응할 수 있었던 점도 중요했습니다.

### 🔑 키워드
> 장애 대응, 모니터링, 로깅, 모니터링 도구, Grafana, Prometheus, 시스템 장애, 로그 분석

<hr>

## 🤷🏻‍♂️ 급격한 사용자 증가로 인해 트래픽이 폭주할 때, 기존 인프라의 어떤 부분을 가장 먼저 개선해야 하며 그 이유는 무엇인가요? 경험에 기반하여 설명해주세요.

### 🙆🏻‍♂️ 답변
> 예전에 사용자 수가 갑자기 증가한 서비스에서 문제를 겪었을 때, 먼저 데이터베이스의 성능을 개선했습니다. 데이터베이스는 가장 중요한 병목 지점이었고, 쿼리 최적화 및 인덱스 튜닝을 통해 처리 속도를 크게 향상시켰습니다. 또한, 캐싱을 적절히 활용하여 데이터베이스 부하를 줄였습니다. 이러한 최적화 덕분에 서비스 가용성을 유지할 수 있었습니다.

### 🔑 키워드
> 트래픽 처리, 인프라, 데이터베이스 최적화, 트래픽 급증 대응, 캐싱 전략

<hr>

## 🤷🏻‍♂️ 실시간 트래픽 급증으로 인해 서버 장애가 발생하는 상황에서, 어떤 방식으로 장애를 최소화하고 빠르게 복구할 수 있는지 본인의 경험을 바탕으로 설명해 주세요.

### 🙆🏻‍♂️ 답변
> 과거 실시간 이벤트 중 트래픽 급증으로 서버가 과부하 상태가 되었던 경험이 있습니다. 사전 준비로 오토스케일링 설정을 해두었고, 장애 발생 시 신속하게 추가 인스턴스를 생성해 트래픽을 분산했습니다. 또한, 캐시 레이어를 적극 활용해 데이터베이스 부하를 줄였습니다. 이러한 조치 덕분에 빠르게 서비스를 안정화할 수 있었습니다.

### 🔑 키워드
> 장애 대응, 모니터링, 로깅, 오토스케일링, 캐시, 트래픽 관리

<hr>

## 🤷🏻‍♂️ TDD를 도입한 프로젝트에서 예상치 못한 어려움을 겪었던 경험이 있나요? 그 상황을 어떻게 해결했는지 구체적으로 설명해주세요.

### 🙆🏻‍♂️ 답변
> 이전 프로젝트에서 TDD를 처음 도입했을 때, 팀 내 개발자 간의 테스트 작성 스타일이 달라 코드 품질이 일관되지 않았습니다. 이를 해결하기 위해 테스트 코드 작성 가이드를 만들고 팀원들과 공유하여 코딩 스타일을 통일했습니다. 또한, 주기적인 코드 리뷰를 통해 가이드를 지속적으로 업데이트하며 팀원의 피드백을 반영했습니다. 이를 통해 코드 품질을 높일 수 있었습니다.
### 🔑 키워드
> 테스트, 코드 품질, TDD, 테스트 가이드, 코드 리뷰

<hr>

## 🤷🏻‍♂️ 실시간 트래픽 급증으로 서비스 응답이 지연되는 문제를 겪었습니다. 모니터링 시스템을 활용하여 어떤 지표를 중점적으로 확인하고, 어떤 대처 방안을 실행했는지 설명해주세요.

### 🙆🏻‍♂️ 답변
> 실시간 트래픽 급증 시, 모니터링 시스템에서 CPU 사용률, 메모리 사용량, 요청 대기 시간 등을 중점적으로 확인했습니다. CPU 사용률과 메모리 사용량이 급증하는 것을 확인하여 먼저 서버의 오토 스케일링을 활성화했습니다. 그와 동시에 요청 대기 시간을 감안하여 캐시를 최적화하고, 급한 요청은 우선적으로 처리하도록 우선순위를 조정했습니다.
### 🔑 키워드
> 장애 대응, 모니터링, 로깅, 오트래픽 급증, 오토 스케일링, 요청 대기 시간

<hr>

## 🤷🏻‍♂️ 복잡한 비즈니스 로직을 구현하는 코드에 대해 단위 테스트를 작성할 때, 발생할 수 있는 일반적인 문제 중 하나는 무엇이며, 이를 어떻게 해결하셨나요?

### 🙆🏻‍♂️ 답변
> 복잡한 비즈니스 로직은 테스트 경계를 정의하는 데 어려움을 줍니다. 제 경험상, 로직을 작게 분리하여 각 모듈을 개별적으로 테스트하는 접근이 효과적이었습니다. <br>이러한 분리로 인해 테스트 범위가 명확해지고, 유지보수가 용이해졌습니다. 또한, 모의 객체(Mock)를 활용하여 외부 의존성을 제거하고 로직 자체에 집중할 수 있었습니다.

### 🔑 키워드
> 테스트 & 코드 품질, 단위 테스트, 모듈화, 모의 객체, 비즈니스 로직, 테스트 경계

<hr>

## 🤷🏻‍♂️ 팀 프로젝트에서 TDD를 도입하려고 할 때, 팀원이 테스트 작성에 익숙하지 않고 거부감을 보입니다. 이런 상황에서 TDD의 효율성을 증명하고 팀원들을 설득하기 위해 어떤 전략을 사용했는지 설명해 주세요.

### 🙆🏻‍♂️ 답변
> TDD 도입을 위해 작은 모듈부터 시작해 성공 사례를 만들었습니다. 우선 팀과 함께 간단한 기능에 대해 테스트를 작성하고, 이를 기반으로 리팩토링 과정을 보여줬습니다. 테스트 덕분에 빠르게 버그를 발견하고 수정할 수 있었고, 생산성이 높아지는 경험을 팀원들이 직접 체감하도록 했습니다. 이를 통해 팀원들이 TDD의 가치와 필요성을 이해하게 되었고, 자발적으로 테스트 작성에 참여하기 시작했습니다.

### 🔑 키워드
> 테스트, 코드 품질, TDD, 팀원 설득, 테스트 작성, 리팩토링, 생산성

<hr>

## 🤷🏻‍♂️ 한 웹 서비스에서 특정 시간대에 트래픽이 급증하여 로드밸런서가 과부하 되어 장애가 발생했습니다. 이 문제를 해결하기 위해 어떻게 인프라를 개선하겠습니까? 구체적인 사례를 들어 설명해주세요.

### 🙆🏻‍♂️ 답변
> 이전 프로젝트에서 유사한 상황을 겪었습니다. 트래픽 패턴을 분석하여 트래픽이 급증하는 시간대에 자동으로 인스턴스를 추가하는 오토 스케일링을 설정했습니다. 또한, DNS 기반의 글로벌 로드 밸런싱을 추가하여 지역별 트래픽 분산을 강화했습니다. 이러한 조치를 통해 트래픽 급증 시에도 안정적인 서비스 운영이 가능했습니다.

### 🔑 키워드
> 트래픽 처리, 인프라, 로드밸런서, 오토 스케일링, 트래픽 분석

<hr>

## 🤷🏻‍♂️ 당신이 참여한 프로젝트에서 특정 디자인 패턴을 적용했을 때, 예상치 못한 문제가 발생했던 경험이 있나요? 그 문제를 어떻게 해결했는지 설명해주세요.

### 🙆🏻‍♂️ 답변
> 이전 프로젝트에서 Observer 패턴을 사용하여 이벤트 알림 시스템을 구축했으나, 구독자가 많아지면서 성능 문제가 발생했습니다. 이를 해결하기 위해 이벤트 큐와 비동기 처리 메커니즘을 추가하여 알림을 병렬로 처리했고, 구독자 수에 따라 스케일링이 가능하도록 구조를 개선했습니다.

### 🔑 키워드
> 클린코드, OOP, 디자인 패턴, Observer 패턴, 성능 최적화, 비동기 처리

<hr>

## 🤷🏻‍♂️ 서비스가 예상치 못한 대규모 트래픽을 받았을 때, 로그 시스템이 오히려 시스템의 부하를 가중시켰던 사례가 있었다면 어떻게 대응했는지 설명해 주세요.

### 🙆🏻‍♂️ 답변
> 예전에 급작스럽게 트래픽이 증가했을 때 로그 시스템이 병목 현상을 일으켜 서비스 응답 시간이 크게 지연된 적이 있었습니다. 이 문제를 해결하기 위해, 로그 레벨을 일시적으로 높여 필수적인 정보만 기록하도록 조정했습니다. 이후 비동기 로깅을 도입하여 메인 프로세스의 부하를 줄였고, 로그 수집 서버를 스케일 아웃하여 안정성을 높였습니다.

### 🔑 키워드
> 장애 대응, 모니터링, 로깅, 로그 레벨, 비동기 로깅, 스케일 아웃

<hr>